
@article{li_evaluating_2020,
	title = {Evaluating Modern {GPU} Interconnect: {PCIe}, {NVLink}, {NV}-{SLI}, {NVSwitch} and {GPUDirect}},
	volume = {31},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2019.2928289},
	shorttitle = {Evaluating Modern {GPU} Interconnect},
	abstract = {High performance multi-{GPU} computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern {GPUs} can be connected and the real impact of state-of-the-art interconnect technology on multi-{GPU} application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern {GPU} interconnects: {PCIe}, {NVLink}-V1, {NVLink}-V2, {NVLink}-{SLI} and {NVSwitch}, from six high-end servers and {HPC} platforms: {NVIDIA} P100-{DGX}-1, V100-{DGX}-1, {DGX}-2, {OLCF}'s {SummitDev} and Summit supercomputers, as well as an {SLI}-linked system with two {NVIDIA} Turing {RTX}-2080 {GPUs}. Based on the empirical evaluation, we have observed four new types of {GPU} communication network {NUMA} effects: three are triggered by {NVLink}'s topology, connectivity and routing, while one is caused by {PCIe} chipset design issue. These observations indicate that, for an application running in a multi-{GPU} node, choosing the right {GPU} combination can impose considerable impact on {GPU} communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-{GPU} performance models, which are vital for {GPU} task allocation, scheduling and migration in a shared environment (e.g., {AI} cloud and {HPC} centers), as well as communication-oriented performance tuning.},
	pages = {94--110},
	number = {1},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan R. and Barker, Kevin J.},
	date = {2020-01},
	note = {Conference Name: {IEEE} Transactions on Parallel and Distributed Systems},
	keywords = {Bandwidth, {GPU}, {GPUDirect}, Graphics processing units, interconnect, {NCCL}, Network topology, {NUMA}, {NVLink}, {NVSwitch}, {PCIe}, Peer-to-peer computing, Performance evaluation, {RDMA}, Routing, {SLI}, Switches, Topology},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Ruili\\Zotero\\storage\\RWCDJ396\\8763922.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Ruili\\Zotero\\storage\\HWT3C7UJ\\Li et al. - 2020 - Evaluating Modern GPU Interconnect PCIe, NVLink, .pdf:application/pdf},
}

@inproceedings{islam_triple-h_2015,
	title = {Triple-H: A Hybrid Approach to Accelerate {HDFS} on {HPC} Clusters with Heterogeneous Storage Architecture},
	doi = {10.1109/CCGrid.2015.161},
	shorttitle = {Triple-H},
	abstract = {{HDFS} (Hadoop Distributed File System) is the primary storage of Hadoop. Even though data locality offered by {HDFS} is important for Big Data applications, {HDFS} suffers from huge I/O bottlenecks due to the tri-replicated data blocks and cannot efficiently utilize the available storage devices in an {HPC} (High Performance Computing) cluster. Moreover, due to the limitation of local storage space, it is challenging to deploy {HDFS} in {HPC} environments. In this paper, we present a hybrid design (Triple-H) that can minimize the I/O bottlenecks in {HDFS} and ensure efficient utilization of the heterogeneous storage devices (e.g. {RAM}, {SSD}, and {HDD}) available on {HPC} clusters. We also propose effective data placement policies to speed up Triple-H. Our design integrated with parallel file system (e.g. Lustre) can lead to significant storage space savings and guarantee fault-tolerance. Performance evaluations show that Triple-H can improve the write and read throughputs of {HDFS} by up to 7x and 2x, respectively. The execution times of data generation benchmarks are reduced by up to 3x. Our design also improves the execution time of the Sort benchmark by up to 40\% over default {HDFS} and 54\% over Lustre. The alignment phase of the Cloudburst application is accelerated by 19\%. Triple-H also benefits the performance of {SequenceCount} and Grep in {PUMA} over both default {HDFS} and Lustre.},
	eventtitle = {2015 15th {IEEE}/{ACM} International Symposium on Cluster, Cloud and Grid Computing},
	pages = {101--110},
	booktitle = {2015 15th {IEEE}/{ACM} International Symposium on Cluster, Cloud and Grid Computing},
	author = {Islam, Nusrat Sharmin and Lu, Xiaoyi and Wasi-ur-Rahman, Md. and Shankar, Dipti and Panda, Dhabaleswar K.},
	date = {2015-05},
	keywords = {Performance evaluation, Big Data, Engines, Fault tolerance, Fault tolerant systems, File systems, {HDFS}, Heterogeneous Storage, {HPC}, Random access memory, Servers},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Ruili\\Zotero\\storage\\LI5ESNND\\7152476.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Ruili\\Zotero\\storage\\65VQTAST\\Islam et al. - 2015 - Triple-H A Hybrid Approach to Accelerate HDFS on .pdf:application/pdf},
}

@book{mccalpin_trends_2015,
	title = {Trends in system cost and performance balances and implications for the future of {HPC}},
	abstract = {For the last decade, {HPC} systems have been dominated by clusters of two-socket commodity x86 servers, typically equipped with a non-commodity high-performance interconnect. Trends in lifecycle costs and prices, hardware technology, several measures of {CPU} and memory performance, and application performance characteristics are presented using several non-traditional perspectives. The evolution of the various ”balances” of the systems over time is discussed --- both in the context of the interaction of application performance with the changing hardware, and in the context of the broader economic environment. Several serious obstacles to maintaining previous performance growth rates are identified and discussed, and it is argued that these are better viewed as architectural and market issues, rather than as fundamental technology issues. It is argued that overcoming these obstacles will require a fundamentally different approach to hardware architecture and programming languages, as well as to system configuration, deployment, and allocation strategies.},
	pagetotal = {1},
	author = {{McCalpin}, John},
	date = {2015-11-15},
	doi = {10.1145/2834899.2834901},
	note = {Pages: 1},
}

@article{dube_future_2021,
	title = {Future of {HPC}: The Internet of Workflows},
	volume = {25},
	issn = {1941-0131},
	doi = {10.1109/MIC.2021.3103236},
	shorttitle = {Future of {HPC}},
	abstract = {Driven by convergence with artificial intelligence and data analytics, increased heterogeneity, and a hybrid cloud/on-premise delivery model, dynamic composition of workflows will be a key design criteria of future high-performance computing ({HPC}) systems. While tightly coupled {HPC} workloads will continue to execute on dedicated supercomputers, other jobs will run elsewhere, including public clouds, and at the edge. Connecting these distributed computing tasks into coherent applications that can perform at scale is what we call “Internet of Workflows.”},
	pages = {26--34},
	number = {5},
	journaltitle = {{IEEE} Internet Computing},
	author = {Dube, Nicolas and Roweth, Duncan and Faraboschi, Paolo and Milojicic, Dejan},
	date = {2021-09},
	note = {Conference Name: {IEEE} Internet Computing},
	keywords = {Cloud computing, Computational modeling, Convergence, Data analysis, Data models, High performance computing, Software, Supercomputers},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Ruili\\Zotero\\storage\\44AZ756U\\9522103.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Ruili\\Zotero\\storage\\W227GVKP\\Dube et al. - 2021 - Future of HPC The Internet of Workflows.pdf:application/pdf},
}

@inproceedings{milojicic_future_2021,
	title = {Future of {HPC}: Diversifying Heterogeneity},
	doi = {10.23919/DATE51398.2021.9474063},
	shorttitle = {Future of {HPC}},
	abstract = {After the end of Dennard scaling and with the imminent end of Moore's Law, it has become challenging to continue scaling {HPC} systems within a given power envelope. This is exacerbated most in large systems, such as high end supercomputers. To alleviate this problem, general purpose is no longer sufficient, and {HPC} systems and components are being augmented with special-purpose hardware. By definition, because of the narrow applicability of specialization, broad supercomputing adoption requires using different heterogeneous components, each optimized for a specific application domain. In this paper, we discuss the impact of the introduced heterogeneity of specialization across the {HPC} stack: interconnects including memory models, accelerators including power and cooling, use cases and applications including {AI}, and delivery models, such as traditional, as-a-Service, and federated. We believe that a stack that supports diversification across hardware and software is required to continue scaling performance and maintaining energy efficiency.},
	eventtitle = {2021 Design, Automation Test in Europe Conference Exhibition ({DATE})},
	pages = {276--281},
	booktitle = {2021 Design, Automation Test in Europe Conference Exhibition ({DATE})},
	author = {Milojicic, Dejan and Faraboschi, Paolo and Dube, Nicolas and Roweth, Duncan},
	date = {2021-02},
	note = {{ISSN}: 1558-1101},
	keywords = {{HPC}, Computational modeling, High performance computing, Software, Supercomputers, accelerators, Artificial intelligence ({AI}), as-a-Service ({aaS}), Background, Cooling, delivery models, diversification), Energy efficiency, Future, Hardware, heterogeneity, High Performance Computing ({HPC}), interconnects, Motivation},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Ruili\\Zotero\\storage\\WKKTUFAP\\9474063.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Ruili\\Zotero\\storage\\Q7L8QRJI\\Milojicic et al. - 2021 - Future of HPC Diversifying Heterogeneity.pdf:application/pdf},
}

@article{ajanovic_pci_nodate,
	title = {{PCI} Express* ({PCIe}*) 3.0 Accelerator Features},
	pages = {10},
	author = {Ajanovic, Jasmin},
	langid = {english},
	file = {Ajanovic - PCI Express (PCIe) 3.0 Accelerator Features.pdf:C\:\\Users\\Ruili\\Zotero\\storage\\AMTDRV3F\\Ajanovic - PCI Express (PCIe) 3.0 Accelerator Features.pdf:application/pdf},
}

@article{lawley_understanding_2014,
	title = {Understanding Performance of {PCI} Express Systems},
	pages = {16},
	author = {Lawley, Jason},
	date = {2014},
	langid = {english},
	file = {Lawley - 2014 - Understanding Performance of PCI Express Systems.pdf:C\:\\Users\\Ruili\\Zotero\\storage\\D7K9G7KI\\Lawley - 2014 - Understanding Performance of PCI Express Systems.pdf:application/pdf},
}

@inproceedings{sharma_pci_2020,
	title = {{PCI} Express® 6.0 Specification at 64.0 {GT}/s with {PAM}-4 signaling: a low latency, high bandwidth, high reliability and cost-effective interconnect},
	doi = {10.1109/HOTI51249.2020.00016},
	shorttitle = {{PCI} Express® 6.0 Specification at 64.0 {GT}/s with {PAM}-4 signaling},
	abstract = {{PCI} Express® ({PCIe}®) specification doubles the data rate every generation in a backwards compatible manner every three years. With {PCIe} 6.0 specification at 64.0 {GT}/s, we will be adopting {PAM}-4 signaling to ensure the channel reach remains the same as {PCIe} 5.0 specification at 32.0 {GT}/s. {PAM}-4 requires a Forward Error Correction ({FEC}) mechanism due to the high {BER}. However, since {PCIe} technology is a Load-Store architecture, the latency adder due to {FEC} must be less than 10ns and ideally 0. We present the new flit-based approach with a light-weight low-latency {FEC} coupled with a low-latency link level retry mechanism. We demonstrate that {PCIe} 6.0 architecture improves the bandwidth efficiency in the range of 0.95 to 1.4, resulting in a net bandwidth increase of 1.9X to 2.8X, depending on the payload size of the packets, in spite of the {FEC} and {CRC} overheads. The latency also decreases in most cases. The reliability ({FIT}) of our approach is demonstrated to less than 10-9. We also present a new power savings strategy that results in power consumption proportional to bandwidth usage without impacting the traffic flow.},
	eventtitle = {2020 {IEEE} Symposium on High-Performance Interconnects ({HOTI})},
	pages = {1--8},
	booktitle = {2020 {IEEE} Symposium on High-Performance Interconnects ({HOTI})},
	author = {Sharma, Debendra Das},
	date = {2020-08},
	note = {{ISSN}: 2332-5569},
	keywords = {Bandwidth, Availability, {BER}, Bit error rate, Correlation, {DUE}, Encoding, {FIT}, Forward error correction, Measurement, {PCI}-Express, Power-Efficiency, Reliability, Replay, Retrain, {SDC}},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Ruili\\Zotero\\storage\\TTYDREZH\\Sharma - 2020 - PCI Express® 6.0 Specification at 64.0 GTs with P.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Ruili\\Zotero\\storage\\PFIHABQX\\9188289.html:text/html},
}

@article{verma_pcie_2017,
	title = {{PCIe} {BUS}: A State-of-the-Art-Review},
	volume = {7},
	doi = {10.9790/4200-0704012428},
	shorttitle = {{PCIe} {BUS}},
	pages = {24--28},
	journaltitle = {{IOSR} Journal of {VLSI} and Signal Processing ({IOSR}-{JVSP})},
	shortjournal = {{IOSR} Journal of {VLSI} and Signal Processing ({IOSR}-{JVSP})},
	author = {Verma, Anuj and Dahiya, Pawan},
	date = {2017-07-12},
	file = {Full Text:C\:\\Users\\Ruili\\Zotero\\storage\\SI4GT7BG\\Verma and Dahiya - 2017 - PCIe BUS A State-of-the-Art-Review.pdf:application/pdf},
}

@inproceedings{nakamura_thorough_2017,
	title = {Thorough analysis of {PCIe} Gen3 communication},
	doi = {10.1109/RECONFIG.2017.8279824},
	abstract = {This article tries a thorough analysis from the physical layer to the transaction layer on {PCIe} Gen3 communication by using {FPGAs}. First, this article shows the performance variation of {PCIe} Gen3 throughput on several system configurations with {FPGAs}. Through the experiment using four kinds of chipsets, eight types of motherboards, and three kinds of {FPGAs}, the performance degradation was about eight percents and fifteen percents in the identical system configurations and different system configurations, respectively. Second, this paper tried to unveil the reason why the performance decreases and to identify where the bottleneck is. The eye scan can investigate the signal quality of a link. Although the comparison of eys-scan results could explain the reason why {PCIe} performance degraded, it did not become the primary reason for {PCIe} performance variance. Then, by the use of a protocol analyzer, this article tried to find out the problem in {PCIe} packets. It found an unintended partitioned transaction, and this seemed to be a significant factor that reduces {PCIe} throughput. Lastly, this article confirms the reason for performance degradation and the performance degradation was organized quantitatively.},
	eventtitle = {2017 International Conference on {ReConFigurable} Computing and {FPGAs} ({ReConFig})},
	pages = {1--6},
	booktitle = {2017 International Conference on {ReConFigurable} Computing and {FPGAs} ({ReConFig})},
	author = {Nakamura, Hiroki and Takayama, Hirotaka and Yamaguchi, Yoshiki and Boku, Taisuke},
	date = {2017-12},
	keywords = {Bridges, Central Processing Unit, Field programmable gate arrays, Physical layer, Protocols, Standards, Throughput},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Ruili\\Zotero\\storage\\WJL63T2I\\Nakamura et al. - 2017 - Thorough analysis of PCIe Gen3 communication.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Ruili\\Zotero\\storage\\3YGR74K4\\8279824.html:text/html},
}

@online{oracle_inc_pci_2010,
	title = {{PCI} Address Domain - Oracle Documentation},
	url = {https://docs.oracle.com/cd/E19253-01/816-4854/hwovr-25/index.html},
	author = {Oracle, Inc.},
	urldate = {2022-02-14},
	date = {2010},
	file = {https\://docs.oracle.com/cd/E19253-01/816-4854/hwovr-25/index.html:C\:\\Users\\Ruili\\Zotero\\storage\\UVV7J8Y8\\index.html:text/html},
}

@online{noauthor_down_nodate,
	title = {Down to the {TLP}: How {PCI} express devices talk (Part I)},
	url = {http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-1},
	shorttitle = {Down to the {TLP}},
	urldate = {2022-02-14},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\FQ9R8FJ7\\pci-express-tlp-pcie-primer-tutorial-guide-1.html:text/html},
}

@online{noauthor_down_nodate-1,
	title = {Down to the {TLP}: How {PCI} express devices talk (Part {II})},
	url = {http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-2},
	shorttitle = {Down to the {TLP}},
	urldate = {2022-02-14},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\XYFPXQ2A\\pci-express-tlp-pcie-primer-tutorial-guide-2.html:text/html},
}

@online{pci-sig_contact_2022,
	title = {Contact Us {\textbar} {PCI}-{SIG}},
	url = {https://pcisig.com/membership/contact-us},
	author = {{PCI}-{SIG}},
	urldate = {2022-02-14},
	date = {2022},
	file = {Contact Us | PCI-SIG:C\:\\Users\\Ruili\\Zotero\\storage\\H2N97NMH\\contact-us.html:text/html},
}

@online{pci-sig_membership_2022,
	title = {Membership {\textbar} {PCI}-{SIG}},
	url = {https://pcisig.com/membership},
	author = {{PCI}-{SIG}},
	urldate = {2022-02-14},
	date = {2022},
	file = {Membership | PCI-SIG:C\:\\Users\\Ruili\\Zotero\\storage\\G4K86VBH\\membership.html:text/html},
}

@article{sun_summarizing_2020,
	title = {Summarizing {CPU} and {GPU} Design Trends with Product Data},
	url = {http://arxiv.org/abs/1911.11313},
	abstract = {Moore's Law and Dennard Scaling have guided the semiconductor industry for the past few decades. Recently, both laws have faced validity challenges as transistor sizes approach the practical limits of physics. We are interested in testing the validity of these laws and reflect on the reasons responsible. In this work, we collect data of more than 4000 publicly-available {CPU} and {GPU} products. We find that transistor scaling remains critical in keeping the laws valid. However, architectural solutions have become increasingly important and will play a larger role in the future. We observe that {GPUs} consistently deliver higher performance than {CPUs}. {GPU} performance continues to rise because of increases in {GPU} frequency, improvements in the thermal design power ({TDP}), and growth in die size. But we also see the ratio of {GPU} to {CPU} performance moving closer to parity, thanks to new {SIMD} extensions on {CPUs} and increased {CPU} core counts.},
	journaltitle = {{arXiv}:1911.11313 [cs]},
	author = {Sun, Yifan and Agostini, Nicolas Bohm and Dong, Shi and Kaeli, David},
	urldate = {2022-02-15},
	date = {2020-07-13},
	eprinttype = {arxiv},
	eprint = {1911.11313},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Ruili\\Zotero\\storage\\J2M9N8FS\\Sun et al. - 2020 - Summarizing CPU and GPU Design Trends with Product.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\KCGCC3YV\\1911.html:text/html},
}

@online{nvidia_cuda_2020,
	title = {{CUDA} 11 Features Revealed},
	url = {https://developer.nvidia.com/blog/cuda-11-features-revealed/},
	abstract = {The new {NVIDIA} A100 {GPU} based on the {NVIDIA} Ampere {GPU} architecture delivers the greatest generational leap in accelerated computing. The A100 {GPU} has revolutionary hardware capabilities and we’re…},
	titleaddon = {{NVIDIA} Developer Blog},
	author = {Nvidia and Pramod, Ramaro},
	urldate = {2022-02-15},
	date = {2020-05-14},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\XRBAQF8T\\cuda-11-features-revealed.html:text/html},
}

@online{intel_what_2022,
	title = {What Is a {GPU}? Graphics Processing Units Defined},
	url = {https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html},
	shorttitle = {What Is a {GPU}?},
	abstract = {Find out what a {GPU} is, how they work, and their uses for parallel processing with a definition and description of graphics processing units.},
	titleaddon = {Intel},
	author = {Intel},
	urldate = {2022-02-15},
	date = {2022},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\Z3STLWK3\\what-is-a-gpu.html:text/html},
}

@online{pci-sig_pci_2011,
	title = {{PCI} Express Architecture Frequently Asked Questions},
	url = {https://web.archive.org/web/20110917001426/http://www.pcisig.com/news_room/faqs/faq_express/pciexpress_faq.pdf},
	author = {{PCI}-{SIG}},
	urldate = {2022-02-17},
	date = {2011-09-17},
	langid = {english},
	file = {PDF Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\SLJN9WPT\\2011 - Wayback Machine.pdf:application/pdf},
}

@online{budruk_pci_2014,
	title = {{PCI} Express Basics},
	url = {https://web.archive.org/web/20140715120034/http://www.pcisig.com/developers/main/training_materials/get_document?doc_id=4e00a39acaa5c5a8ee44ebb07baba982e5972c67},
	author = {Budruk, Ravi},
	urldate = {2022-02-19},
	date = {2014-07-15},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\E9YGNDWN\\get_document.html:text/html},
}

@inproceedings{mcclanahan_history_2010,
	title = {History and Evolution of {GPU} Architecture},
	url = {https://www.semanticscholar.org/paper/History-and-Evolution-of-GPU-Architecture-A-Paper-McClanahan/247980e834f1c8f684d85067402f950930e6af91},
	pages = {7},
	author = {{McClanahan}, Chris},
	urldate = {2022-02-27},
	date = {2010},
	langid = {english},
	file = {McClanahan - History and Evolution of GPU Architecture.pdf:C\:\\Users\\Ruili\\Zotero\\storage\\XXE8GQKM\\McClanahan - History and Evolution of GPU Architecture.pdf:application/pdf},
}

@online{nvidia_cuda_2017,
	title = {{CUDA} Zone},
	url = {https://developer.nvidia.com/cuda-zone},
	abstract = {{CUDA} Zone {CUDA}® is a parallel computing platform and programming model developed by {NVIDIA} for general computing on graphical processing units ({GPUs}). With {CUDA}, developers are able to dramatically speed up computing applications by harnessing the power of {GPUs}. In {GPU}-accelerated applications, the sequential part of the workload runs on the {CPU} – which is optimized for single-threaded performance – while the compute intensive portion of the application runs on thousands of {GPU} cores in parallel.},
	titleaddon = {{NVIDIA} Developer},
	author = {Nvidia},
	urldate = {2022-02-19},
	date = {2017-07-18},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\YYSHPDRX\\cuda-zone.html:text/html},
}

@online{nvidia_nvidia_2020,
	title = {{NVIDIA} {GeForce} {RTX} 3090 Graphics Card},
	url = {https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090/},
	abstract = {A staggering 24 {GB} of G6X memory, all to deliver the ultimate gaming experience},
	author = {Nvidia},
	urldate = {2022-02-19},
	date = {2020},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\QM25CTKS\\rtx-3090.html:text/html},
}

@online{micron_technology_inc_ram_2022,
	title = {{RAM} Memory Speeds \& Compatibility {\textbar} Crucial.com},
	url = {https://www.crucial.com/support/memory-speeds-compatability},
	abstract = {Better understand your computer's {RAM} speeds \& the compatibility of different types of memory to work out how to get the most from your desktop or laptop.},
	titleaddon = {Crucial},
	author = {Micron Technology, Inc.},
	urldate = {2022-02-19},
	date = {2022},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\69GCPW72\\memory-speeds-compatability.html:text/html},
}

@online{ibm_what_2022,
	title = {What is {HPC}? Introduction to high-performance computing {\textbar} {IBM}},
	url = {https://www.ibm.com/topics/hpc},
	shorttitle = {What is {HPC}?},
	abstract = {High-performance computing ({HPC}) technology harnesses the power of supercomputers or computer clusters to solve complex problems requiring massive computation.},
	author = {{IBM}},
	urldate = {2022-02-20},
	date = {2022},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\9WSBARRY\\hpc.html:text/html},
}

@online{iowa_state_university_what_2020,
	title = {What is an {HPC} cluster {\textbar} High Performance Computing},
	url = {https://www.hpc.iastate.edu/guides/introduction-to-hpc-clusters/what-is-an-hpc-cluster},
	author = {Iowa State University},
	urldate = {2022-02-20},
	date = {2020},
	file = {What is an HPC cluster | High Performance Computing:C\:\\Users\\Ruili\\Zotero\\storage\\V39FJFE3\\what-is-an-hpc-cluster.html:text/html},
}

@online{intel_product_2022,
	title = {Product Specifications - i9 12900k},
	url = {https://www.intel.com/content/www/us/en/products/sku/134599/intel-core-i912900k-processor-30m-cache-up-to-5-20-ghz.html},
	abstract = {quick reference guide including specifications, features, pricing, compatibility, design documentation, ordering codes, spec codes and more.},
	author = {Intel},
	urldate = {2022-02-20},
	date = {2022},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\B3MXRGWE\\intel-core-i912900k-processor-30m-cache-up-to-5-20-ghz.html:text/html},
}

@online{intel_what_2022-1,
	title = {What Is Thunderbolt 4?},
	url = {https://www.intel.com/content/www/us/en/gaming/resources/upgrade-gaming-accessories-thunderbolt-4.html},
	abstract = {Learn about how Thunderbolt 4 ports, cables, and accessories can extend the I/O capabilities of your gaming setup.},
	titleaddon = {Intel},
	author = {Intel},
	urldate = {2022-02-20},
	date = {2022},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\SBXFHM5M\\upgrade-gaming-accessories-thunderbolt-4.html:text/html},
}

@online{kingston_understanding_2017,
	title = {Understanding {SSD} Technology: {NVMe}, {SATA}, M.2 - Kingston Technology},
	url = {https://www.kingston.com/germany/en/community/articledetail/articleid/48543},
	shorttitle = {Understanding {SSD} Technology},
	abstract = {What is M.2?  What is {NVMe}?  How are these related to {SATA}, {AHCI} and what is the speed difference? We break down each acronym in our infographic to help you better understand how and why the latest {SSD} technology is faster and better.},
	titleaddon = {Kingston Technology Company},
	author = {Kingston},
	urldate = {2022-02-20},
	date = {2017-02},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\PLP3C89V\\48543.html:text/html},
}

@book{jackson_pci_2012,
	location = {Monument, Colo.},
	edition = {1st ed},
	title = {{PCI} Express technology: comprehensive guide to generations 1.x, 2.x, 3.0},
	isbn = {978-0-9770878-6-0},
	series = {{MindShare} technology series},
	shorttitle = {{PCI} Express technology},
	pagetotal = {986},
	publisher = {{MindShare}},
	author = {Jackson, Mike and Budruk, Ravi and Winkles, Joe and Anderson, Don},
	date = {2012},
	note = {{OCLC}: ocn824814290},
	keywords = {Buses, Computer architecture, Microcomputers},
	file = {Jackson and Budruk - PCI Express Technology.pdf:C\:\\Users\\Ruili\\Zotero\\storage\\4D2UVV8F\\Jackson and Budruk - PCI Express Technology.pdf:application/pdf},
}

@online{nvidia_cuda_2022,
	title = {{CUDA} C++ Programming Guide},
	url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html},
	abstract = {The programming guide to the {CUDA} model and interface.},
	type = {documentation},
	author = {Nvidia},
	urldate = {2022-02-22},
	date = {2022-02-03},
	langid = {english},
	note = {Archive Location: Programming Guides},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\4JABJCS2\\index.html:text/html},
}

@inproceedings{shalf_hpc_2019,
	title = {{HPC} Interconnects at the End of Moore's Law},
	abstract = {The tapering of lithography advances that have been associated with Moore's Law will substantially change requirements for future interconnect architectures for large-scale datacenters and {HPC} systems. Architectural specialization is creating new datacenter requirements such as emerging accelerator technologies for machine learning workloads and rack disaggregation strategies will push the limits of current interconnect technologies. Whereas photonic technologies are often sold on the basis of higher bandwidth and energy efficiency (e.g. lower picojoules per bit), these emerging workloads and technology trends will shift the emphasis to other metrics such as bandwidth density (as opposed to bandwidth alone) and reduced latency, and performance consistency. Such metrics cannot be accomplished with device improvements alone, but require a systems view of photonics in datacenters.},
	eventtitle = {2019 Optical Fiber Communications Conference and Exhibition ({OFC})},
	pages = {1--3},
	booktitle = {2019 Optical Fiber Communications Conference and Exhibition ({OFC})},
	author = {Shalf, John},
	date = {2019-03},
	keywords = {Bandwidth, Hardware, Computer architecture, Fabrics, Integrated circuit interconnections, Photonics, Silicon},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Ruili\\Zotero\\storage\\WGRYD7J8\\Shalf - 2019 - HPC Interconnects at the End of Moore's Law.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Ruili\\Zotero\\storage\\LU4ACHG8\\8696552.html:text/html},
}

@online{nvidia_nvidia_2011,
	title = {{NVIDIA} Management Library ({NVML})},
	url = {https://developer.nvidia.com/nvidia-management-library-nvml},
	abstract = {A C-based {API} for monitoring and managing various states of the {NVIDIA} {GPU} devices. It provides a direct access to the queries and commands exposed via nvidia-smi. The runtime version of {NVML} ships with the {NVIDIA} display driver, and the {SDK} provides the appropriate header, stub libraries and sample applications. Each new version of {NVML} is backwards compatible and is intended to be a platform for building 3rd party applications. Query-able states includes: {ECC} error counts: Both correctable single bit and detectable double bit errors are reported.},
	titleaddon = {{NVIDIA} Developer},
	author = {Nvidia},
	urldate = {2022-02-25},
	date = {2011-10-21},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\RRJXRB26\\nvidia-management-library-nvml.html:text/html},
}

@online{nvidia_unified_2017,
	title = {Unified Memory for {CUDA} Beginners},
	url = {https://developer.nvidia.com/blog/unified-memory-cuda-beginners/},
	abstract = {This post introduces {CUDA} programming with Unified Memory, a single memory address space that is accessible from any {GPU} or {CPU} in a system.},
	titleaddon = {{NVIDIA} Technical Blog},
	author = {Nvidia and Harris, Mark},
	urldate = {2022-02-25},
	date = {2017-06-20},
	langid = {american},
}

@online{nvidia_how_2012,
	title = {How to Optimize Data Transfers in {CUDA} C/C++},
	url = {https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/},
	abstract = {In the previous three posts of this {CUDA} C \& C++ series we laid the groundwork for the major thrust of the series: how to optimize {CUDA} C/C++ code. In this and the following post we begin our…},
	titleaddon = {{NVIDIA} Technical Blog},
	author = {Nvidia and Harris, Mark},
	urldate = {2022-02-25},
	date = {2012-12-05},
	langid = {american},
}

@article{moore_cramming_1965,
	title = {Cramming more components onto integrated circuits},
	volume = {38},
	pages = {6},
	number = {8},
	author = {Moore, Gordon E},
	date = {1965},
	langid = {english},
	file = {Moore - 1965 - Cramming more components onto integrated circuits.pdf:C\:\\Users\\Ruili\\Zotero\\storage\\6F4TU9D4\\Moore - 1965 - Cramming more components onto integrated circuits.pdf:application/pdf},
}

@article{dennard_design_1974,
	title = {Design of ion-implanted {MOSFET}'s with very small physical dimensions},
	volume = {9},
	issn = {0018-9200, 1558-173X},
	url = {https://ieeexplore.ieee.org/document/1050511/},
	doi = {10.1109/JSSC.1974.1050511},
	pages = {256--268},
	number = {5},
	journaltitle = {{IEEE} Journal of Solid-State Circuits},
	shortjournal = {{IEEE} J. Solid-State Circuits},
	author = {Dennard, R.H. and Gaensslen, F.H. and Yu, Hwa-Nien and Rideout, V.L. and Bassous, E. and {LeBlanc}, A.R.},
	urldate = {2022-02-25},
	date = {1974-10},
	file = {Submitted Version:C\:\\Users\\Ruili\\Zotero\\storage\\JGEYKF38\\Dennard et al. - 1974 - Design of ion-implanted MOSFET's with very small p.pdf:application/pdf},
}

@article{shalf_computing_2015,
	title = {Computing beyond Moore's Law},
	volume = {48},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/7368023/},
	doi = {10.1109/MC.2015.374},
	pages = {14--23},
	number = {12},
	journaltitle = {Computer},
	shortjournal = {Computer},
	author = {Shalf, John M. and Leland, Robert},
	urldate = {2022-02-25},
	date = {2015-12},
	file = {Full Text:C\:\\Users\\Ruili\\Zotero\\storage\\726ST2LP\\Shalf and Leland - 2015 - Computing beyond Moore's Law.pdf:application/pdf},
}

@online{tsmc_5nm_2022,
	title = {5nm Technology - Taiwan Semiconductor Manufacturing Company Limited},
	url = {https://www.tsmc.com/english/dedicatedFoundry/technology/logic/l_5nm},
	abstract = {{TSMC}’s 5nm (N5) Fin Field-Effect Transistor ({FinFET}) technology successfully entered volume production in the second quarter of 2020 and experienced a strong ramp in the second half of 2020.},
	author = {{TSMC}},
	urldate = {2022-02-25},
	date = {2022},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\UBKAJ63T\\l_3nm.html:text/html},
}

@online{micron_technology_inc_gddr6x_2022,
	title = {{GDDR}6X},
	url = {https://www.micron.com/products/ultra-bandwidth-solutions/gddr6x},
	abstract = {Memory Reimagined. Choose the innovative {PAM}4 signaling technology in Micron’s {GDDR}6X memory to double your data rate, delivering unprecedented graphics memory performance to feed the most data-hungry workloads. Get the world’s fastest discrete memory for your gaming, professional visualization and {AI} workloads.},
	author = {Micron Technology, Inc.},
	urldate = {2022-02-26},
	date = {2022},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\QHYKIUQV\\gddr6x.html:text/html},
}

@online{nvidia_cuda_2022-1,
	title = {{CUDA} Toolkit Documentation: Memory Management},
	url = {https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html},
	type = {{cppModule}},
	author = {Nvidia},
	urldate = {2022-02-27},
	date = {2022-02-22},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\38HNC26C\\group__CUDART__MEMORY.html:text/html},
}

@online{noauthor_malloc_2021,
	title = {malloc - C++ Reference},
	url = {https://www.cplusplus.com/reference/cstdlib/malloc/},
	urldate = {2022-02-27},
	date = {2021},
	file = {malloc - C++ Reference:C\:\\Users\\Ruili\\Zotero\\storage\\TCSKMRSC\\malloc.html:text/html},
}

@online{noauthor_size_t_2021,
	title = {size\_t - C++ Reference},
	url = {https://www.cplusplus.com/reference/cstdlib/size_t/},
	urldate = {2022-02-27},
	date = {2021},
	file = {size_t - C++ Reference:C\:\\Users\\Ruili\\Zotero\\storage\\XDVLRVRV\\size_t.html:text/html},
}

@online{nvidia_device_2021,
	title = {NVML Device Queries},
	url = {http://docs.nvidia.com/deploy/nvml-api/index.html},
	type = {{cppModule}},
	author = {Nvidia},
	urldate = {2022-02-27},
	date = {2021-07-29},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Ruili\\Zotero\\storage\\8HQCCJ5R\\group__nvmlDeviceQueries.html:text/html},
}